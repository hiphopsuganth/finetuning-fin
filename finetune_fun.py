# -*- coding: utf-8 -*-
"""finetune-fun.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHtMiMxIsdXFN2zpW5r9G9_eB0g6xWUI
"""

!pip install -U datasets

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install  transformers==4.19.2
# !pip install rouge_score

from datasets import load_metric
import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/codecycle/WikiHow.csv')
df.head()

print(df.shape)
df = df.dropna()
print(df.shape)

print(df.shape)
df = df.drop_duplicates()
print(df.shape)

df['length'] = df.paragraph.map(lambda x: len(x.split(" ")))

numofwords = df.length
from matplotlib import pyplot as plt

fig = plt.figure(figsize =(5,5))
plt.hist(numofwords.to_numpy(), bins = [0,50,100,200,300,500,1000])


plt.title("word  count distribution")

plt.show()

tempdf = df[df.length < 200]
print(tempdf.shape)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("allenai/led-base-16384")

max_input_length =1024
max_output_length = 64
batch_size = 16

def process_data_to_model_inputs(batch):
    # tokenize the inputs and labels
    inputs = tokenizer(
        batch["paragraph"],
        padding="max_length",
        truncation=True,
        max_length=max_input_length,
    )
    outputs = tokenizer(
        batch["paragraph"],
        padding="max_length",
        truncation=True,
        max_length=max_output_length,
    )

    batch["input_ids"] = inputs.input_ids
    batch["attention_mask"] = inputs.attention_mask

    batch["global_attention_mask"] = len(batch["input_ids"]) * [
        [0 for _ in range(len(batch["input_ids"][0]))]
    ]

    batch["global_attention_mask"][0][0] = 1
    batch["labels"] = outputs.input_ids

    batch["labels"] =[
         [-100 if token == tokenizer.pad_token_id else token for token in labels]
         for labels in batch["labels"]
    ]
    return batch

import numpy as np
train,validate,test = np.split(tempdf.sample(frac=1, random_state=42), [int(.6*len(tempdf)), int(.7*len(tempdf))])
print(train.shape)
print(validate.shape)
print(test.shape)

validate = validate[:20]
validate.shape

from datasets import Dataset
train_dataset = Dataset.from_pandas(train)
validate_dataset = Dataset.from_pandas(validate)


train_dataset = train_dataset.map(
    process_data_to_model_inputs,
    batched=True,
    batch_size=batch_size,
    remove_columns=["title","heading","paragraph","length","__index_level_0__"],
)
validate_dataset = validate_dataset.map(
    process_data_to_model_inputs,
    batched=True,
    batch_size=batch_size,
    remove_columns=["title","heading","paragraph","length","__index_level_0__"],
)

train_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"],
)
validate_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"],
)

from transformers import AutoModelForSeq2SeqLM
led = AutoModelForSeq2SeqLM.from_pretrained("allenai/led-base-16384",gradient_checkpointing=True,use_cache=False)
led.config.num_beams = 2
led.config.max_length = 64
led.config.min_length = 2
led.config.length_penalty = 2.0
led.config.early_stopping = True
led.config.no_repeat_ngram_size = 3
rouge = load_metric("rouge")


def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    rouge_output = rouge.compute(
        predictions=pred_str, references=label_str, rouge_types=["rouge2"]
    )["rouge2"].mid

    return {
        "rouge2_precision": round(rouge_output.precision, 4),
        "rouge2_recall": round(rouge_output.recall, 4),
        "rouge2_fmeasure": round(rouge_output.fmeasure,4),
    }

from transformers import Seq2SeqTrainer , Seq2SeqTrainingArguments
import transformers
transformers.logging.set_verbosity_info()
training_args = Seq2SeqTrainingArguments(
      predict_with_generate = True,
      evaluation_strategy = "steps",
      per_device_train_batch_size = batch_size,
      per_device_eval_batch_size = batch_size,
      output_dir = "./",
      logging_steps = 5,
      eval_steps = 10,
      save_steps = 10,
      save_total_limit = 2,
      gradient_accumulation_steps =4,
      num_train_epochs = 10
     )

trainer = Seq2SeqTrainer(
      model = led,
      args = training_args,
      compute_metrics = compute_metrics,
      train_dataset = train_dataset,
      eval_dataset = validate_dataset,
      tokenizer = tokenizer
  )

# trainer.train()

import pandas as pd
sample_paragraph = '''The reason why I loved the top-down culture at Apple is that important decisions
are taken faster. Having an expert giving you green light or not keeps the
momentum. How many times in a bottom-up culture do we spend weeks and
weeks, sometimesThe reason why I loved the top-down culture at Apple is that important decisions
are taken faster. Having an expert giving you green light or not keeps the
momentum. How many times in a bottom-up culture do we spend weeks and
weeks, sometimes even months, trying to get alignment with +10 people, because
every single person needs to agree with the point of view? It is exhausting. So
again, my experience is that having that one leader to look up to to help guide
decisions is time-saving, it helps us focus on the design craft, instead of project
management even months, trying to get alignment with +10 people, because
every single person needs to agree with the point of view? It is exhausting. So
again, my experience is that having that one leader to look up to to help guide
decisions is time-saving, it helps us focus on the design craft, instead of project
management '''

data = [sample_paragraph]
df = pd.DataFrame(data,columns =['paragraph'])
df['paragraph'][0]
from datasets import Dataset
df_test = Dataset.from_pandas(df)
df_test

from datasets import load_metric
import torch
from datasets import load_dataset, load_metric
from transformers import LEDTokenizer, LEDForConditionalGeneration


tokenizer = LEDTokenizer.from_pretrained("/content/drive/MyDrive/codecycle/checkpoint-80")
model = LEDForConditionalGeneration.from_pretrained("/content/drive/MyDrive/codecycle/checkpoint-80").to("cuda").half()


def generate_answer(batch):
  inputs_dict = tokenizer(
      batch["paragraph"],
      max_length=512,
      padding="max_length",
      truncation=True,
      return_tensors="pt",
  )
  input_ids = inputs_dict.input_ids.to("cuda")
  attention_mask = inputs_dict.attention_mask.to("cuda")
  global_attention_mask = torch.zeros_like(input_ids)
  global_attention_mask[:, 0] = 1

  predicted_abstract_ids = model.generate(
      input_ids=input_ids,
      attention_mask=attention_mask,
      global_attention_mask=global_attention_mask)
  batch["predicted_abstract"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)
  return batch

result = df_test.map(generate_answer,batched=True,batch_size=2)

result["predicted_abstract"]

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

def summarize_text(text):

    inputs = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=max_input_length).to(device)

    summary_ids = model.generate(inputs, max_length=max_output_length, min_length=2, length_penalty=2.0, num_beams=2, early_stopping=True)

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

print("Summary:", summarize_text(sample_paragraph))
print("Tags:", generate_tags(sample_paragraph))

